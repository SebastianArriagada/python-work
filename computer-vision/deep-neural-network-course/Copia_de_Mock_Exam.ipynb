{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Mock Exam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mock Exam**"
      ],
      "metadata": {
        "id": "hY88PZuGRlvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This notebook is to help you prepare for the exam. It contains a task similar to what will be on the first part of the exam. Although this is just a simpler version of the actual exam, what is used to solve this task should provide you with basic knowledge needed for the exam (part one). I also included the solutions."
      ],
      "metadata": {
        "id": "994g3vW4RpXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name**:\n",
        "\n",
        "**Neptun code:**"
      ],
      "metadata": {
        "id": "VkZXq9lCnsKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task Description"
      ],
      "metadata": {
        "id": "krzdzOL0Sejg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your task is to implement an Encoder-Decoder structure and the forward functions. \n",
        "\n",
        "#### Afterwards, make sure to run cell code number 1.3. and 1.6. to know if your implementation is correct.\n",
        "\n",
        "#### This task should be **SOLVED IN 1 HOUR** and submitted to Canvas (download the .ipynb file). Please note that after 1 hour, the Canvas exam assignment will be closed and you cannot submit your solution. "
      ],
      "metadata": {
        "id": "vjArI5jdUlLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "8Qn8jLgy81ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **NO GPU IS NEEDED for this task**. No training nor any computationally expensive operation will be performed. This notebook runs on any computer using a cpu."
      ],
      "metadata": {
        "id": "ztfR3038icfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #make sure that you are using GPU acceleration\n",
        "#device"
      ],
      "metadata": {
        "id": "ExK30uAjUaz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Architecture"
      ],
      "metadata": {
        "id": "3jPxTombVv9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Please keep in mind that this architecture is purely imagined and should not correspond to any existing model / architecture. You will not find it on the internet."
      ],
      "metadata": {
        "id": "vchO-YTwe38A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please right click the image and \"Open image in a new tab\" to view it better with zoom. Or download it from here: https://drive.google.com/file/d/1up5D0ikCBUt5T_RVbChEsz3Xb96akDXO/view?usp=sharing\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1up5D0ikCBUt5T_RVbChEsz3Xb96akDXO)"
      ],
      "metadata": {
        "id": "nwkZf0__hy2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1. Implement the encoder"
      ],
      "metadata": {
        "id": "wtvGCndKhzhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wwn11gARdQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b758c4eb-1eee-419b-d12f-5c8c35c3c93a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 512, 5, 5])\n",
            "torch.Size([10, 12800])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, bottleneck_size = 128):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        \n",
        "        # GET the pretrained vgg16 model\n",
        "        \n",
        "        vgg16 = models.vgg16(pretrained = True)\n",
        "\n",
        "        # REMOVE the \"classifier\" layers. In order to know how to solve this,\n",
        "        # printing the architecture structure might be useful\n",
        "\n",
        "        modules = list(vgg16.children())[:-2]\n",
        "        self.vgg16 = nn.Sequential(*modules)\n",
        "        \n",
        "\n",
        "        # DEFINE the convolutions. You need to know the output of the VGG16 without the last layer\n",
        "        # in order to define the input channels. Again, printing the architecture might be useful\n",
        "\n",
        "        self.conv256_3x3 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=\"valid\", dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "        \n",
        "        \n",
        "\n",
        "        # DEFINE a fully connected layer that receives the combined convolutions' features as input features\n",
        "        # and outputs the bottleneck_size.\n",
        "        # You need to know the output size of the combined convolutions, reshaped into a vector (batch_size, combined features)\n",
        "        self.fc = nn.Linear(12800, 128)\n",
        "\n",
        "        \n",
        "        # DEFINE a dropout layer with probability 0.3\n",
        "        self.dropOut = nn.Dropout(0.3)\n",
        "        \n",
        "        # DEFINE a ReLU activation layer\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \n",
        "        # GET the features from the VGG16\n",
        "        features = self.vgg16(images)\n",
        "        \n",
        "\n",
        "        # SEND the features to the two branches\n",
        "        encoder_a = torch.nn.Sequential(\n",
        "            self.conv256_3x3,\n",
        "            self.relu,\n",
        "        )\n",
        "        \n",
        "        encoder_b = torch.nn.Sequential(\n",
        "            self.conv256_3x3,\n",
        "            self.dropOut,\n",
        "        )\n",
        "        \n",
        "        #COMBINE the branches\n",
        "        cat = torch.cat([encoder_a(features) , encoder_b(features)], dim=1, out=None)\n",
        "\n",
        "        print(cat.shape)\n",
        "\n",
        "        # RESHAPE the combined output of the convolutions so that it can be fed to the linear layer.\n",
        "        # Alternatively you can FLATTEN it.\n",
        "        x = torch.flatten(cat, 1)\n",
        "        print(x.shape)\n",
        "\n",
        "        # GET the bottleneck from the fully connected layer\n",
        "        bottleneck = self.fc(x) #change the None to the actual layer\n",
        "        \n",
        "        return bottleneck\n",
        "\n",
        "encoder = EncoderCNN()\n",
        "encoded = encoder(torch.randn(10,3,224,224))\n",
        "encoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2. Solution"
      ],
      "metadata": {
        "id": "RKVSTPi6h8W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, bottleneck_size = 128):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        \n",
        "        # GET the pretrained vgg16 model\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        \n",
        "        # REMOVE the \"classifier\" layers. In order to know how to solve this,\n",
        "        # printing the architecture structure might be useful\n",
        "        modules = list(vgg16.children())[:-1]\n",
        "        self.vgg16 = nn.Sequential(*modules)\n",
        "\n",
        "        # DEFINE the convolutions. You need to know the output of the VGG16 without the last layer\n",
        "        # in order to define the input channels. Again, printing the architecture might be useful\n",
        "        self.conv2d_A = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding='valid')\n",
        "        self.conv2d_B = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding='valid')\n",
        "\n",
        "        # DEFINE a fully connected layer that receives the combined convolutions' features as input features\n",
        "        # and outputs the bottleneck_size.\n",
        "        # You need to know the output size of the combined convolutions, reshaped into a vector (batch_size, combined features)\n",
        "        self.linear = nn.Linear(in_features=12800, out_features=bottleneck_size)\n",
        "        \n",
        "        # DEFINE a dropout layer with probability 0.3\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        \n",
        "        # DEFINE a ReLU activation layer\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \n",
        "        # GET the features from the VGG16\n",
        "        features = self.vgg16(images)\n",
        "\n",
        "        # SEND the features to the two branches\n",
        "        branch_A = self.conv2d_A(features)\n",
        "        branch_A = self.relu(branch_A)\n",
        "\n",
        "        branch_B = self.conv2d_B(features)\n",
        "        branch_B = self.dropout(branch_B)\n",
        "        \n",
        "        #COMBINE the branches\n",
        "        combined = torch.cat((branch_A, branch_B), axis=1)\n",
        "\n",
        "        # RESHAPE the combined output of the convolutions so that it can be fed to the linear layer.\n",
        "        # Alternatively you can FLATTEN it.\n",
        "        combined = torch.flatten(combined, start_dim=1) #or combined = combined.view(-1,512*5*5)\n",
        "\n",
        "        # GET the bottleneck from the fully connected layer\n",
        "        bottleneck = self.linear(combined) #change the None to the actual layer\n",
        "        \n",
        "        return bottleneck"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QFb4530ch-kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3. Test your implementation.\n",
        "Expected output \n",
        "\n",
        "torch.Size( [10, 128] )"
      ],
      "metadata": {
        "id": "DJqVLXTqJR7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderCNN()\n",
        "encoded = encoder(torch.randn(10,3,224,224))\n",
        "encoded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "aLuor1bCJRKf",
        "outputId": "bed2f882-d71f-4ef2-d3c5-0ee5dadcca0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cd4b0a9314d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f59601cf915a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#COMBINE the branches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoder_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# RESHAPE the combined output of the convolutions so that it can be fed to the linear layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 25088, 3, 3], expected input[10, 512, 7, 7] to have 25088 channels, but got 512 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4. Implement the Decoder"
      ],
      "metadata": {
        "id": "_X_FRBfjh_aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size=128, hidden_size=512, vocab_size=1000):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # DEFINE the LSTM Cell, it takes the size of the embedding as input size and has a hidden size \n",
        "        self.lstm_cell = nn.LSTMCell(embed_size, hidden_size) \n",
        "    \n",
        "        # DEFINE the fully connected layer that takes as input features the output of the LSTM \n",
        "        # and outputs features of the size of the vocabulary\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    \n",
        "        # DEFINE the embedding layer with the size of the vocabulary and a dimension of embed size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    \n",
        "        # DEFINE a sigmoid activation\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, encoded, captions):\n",
        "        # GET the batch size from the encoded input\n",
        "        batch_size = encoded.size(0)\n",
        "        \n",
        "        # INITIALIZE the hidden and cell states to zeros taking into consideration the batch size\n",
        "        hidden_state = torch.zeros(batch_size, self.hidden_size)\n",
        "        cell_state = torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "        # DONE for you\n",
        "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size))\n",
        "\n",
        "        # APPLY embeddings to the captions. Please use captions.to(torch.int64) to avoid errors\n",
        "        captions_embed = self.embedding(captions.to(torch.int64))\n",
        "  \n",
        "        # PASS THE CAPTION TO THE MODEL, WORD BY WORD\n",
        "        for t in range(captions.size(1)):\n",
        "\n",
        "            # for the first time step the INPUT IS THE ENCODED VECTOR.\n",
        "            # don't forget to GET the hidden state and cell state\n",
        "            if t == 0:\n",
        "                hidden_state, cell_state = self.lstm_cell(encoded, (hidden_state, cell_state))\n",
        "                \n",
        "            # for the 2nd+ time step the INPUT IS THE EMBEDDED CAPTIONS.\n",
        "            # take into consideration only the time stamp axis (tokens) and not the batch size and the size [:,???,:]\n",
        "            # don't forget to GET the hidden state and cell state\n",
        "            else:\n",
        "                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
        "            \n",
        "            # GET the output from the fully connected layer\n",
        "            out = self.fc(hidden_state)\n",
        "\n",
        "            # APPLY the activation function. Call it \"out\"\n",
        "            out = self.sigmoid(out) #replace None with the actual layer\n",
        "            \n",
        "            # DONE for you\n",
        "            outputs[:, t, :] = out\n",
        "    \n",
        "        return outputs\n",
        "decoder = DecoderRNN()\n",
        "captions = torch.abs(torch.randn(10,1000))\n",
        "decoded = decoder(encoded,captions)\n",
        "decoded.shape"
      ],
      "metadata": {
        "id": "qJWygadKh93J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553734ba-9cae-4cef-aecd-38d6a5a32a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1000, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5. Solution"
      ],
      "metadata": {
        "id": "SMaaNctbiCdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size=128, hidden_size=512, vocab_size=1000):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # DEFINE the LSTM Cell, it takes the size of the embedding as input size and has a hidden size \n",
        "        self.lstm_cell = nn.LSTMCell(input_size=self.embed_size, hidden_size=self.hidden_size)\n",
        "    \n",
        "        # DEFINE the fully connected layer that takes as input features the output of the LSTM \n",
        "        # and outputs features of the size of the vocabulary\n",
        "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
        "    \n",
        "        # DEFINE the embedding layer with the size of the vocabulary and a dimension of embed size\n",
        "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
        "    \n",
        "        # DEFINE a sigmoid activation\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, encoded, captions):\n",
        "        # GET the batch size from the encoded input\n",
        "        batch_size = encoded.size(0)\n",
        "        \n",
        "        # INITIALIZE the hidden and cell states to zeros taking into consideration the batch size\n",
        "        hidden_state = torch.zeros((batch_size, self.hidden_size))\n",
        "        cell_state = torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "        # DONE for you\n",
        "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size))\n",
        "\n",
        "        # APPLY embeddings to the captions. Please use captions.to(torch.int64) to avoid errors\n",
        "        captions_embed = self.embed(captions.to(torch.int64))\n",
        "\n",
        "        # PASS THE CAPTION TO THE MODEL, WORD BY WORD\n",
        "        for t in range(captions.size(1)):\n",
        "\n",
        "            # for the first time step the INPUT IS THE ENCODED VECTOR.\n",
        "            # don't forget to GET the hidden state and cell state\n",
        "            if t == 0:\n",
        "                hidden_state, cell_state = self.lstm_cell(encoded, (hidden_state, cell_state))\n",
        "                \n",
        "            # for the 2nd+ time step the INPUT IS THE EMBEDDED CAPTIONS.\n",
        "            # take into consideration only the time stamp axis (tokens) and not the batch size and the size [:,???,:]\n",
        "            # don't forget to GET the hidden state and cell state\n",
        "            else:\n",
        "                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
        "            \n",
        "            # GET the output from the fully connected layer\n",
        "            out = self.fc_out(hidden_state)\n",
        "\n",
        "            # APPLY the activation function. Call it \"out\"\n",
        "            out = self.sigmoid(out)\n",
        "            \n",
        "            # DONE for you\n",
        "            outputs[:, t, :] = out\n",
        "    \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "m-6va_YbjGEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.6. Test your implementation\n",
        "This requires the Encoder to be defined and its output to be stored in an \"encoded\" variable (cell 1.3. does that).\n",
        "\n",
        "Expected output\n",
        "\n",
        "torch.Size( [10, 1000, 1000] )\n"
      ],
      "metadata": {
        "id": "SBZCuuzuM-U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = DecoderRNN()\n",
        "captions = torch.abs(torch.randn(10,1000))\n",
        "decoded = decoder(encoded,captions)\n",
        "decoded.shape"
      ],
      "metadata": {
        "id": "2152N0b4NBT-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}